{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yel3s22GAi2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "2783d929-38fa-400b-8d1c-cfcdfd0a2e58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: pillow 10.2.0\n",
            "Uninstalling pillow-10.2.0:\n",
            "  Successfully uninstalled pillow-10.2.0\n",
            "Collecting pillow==10.2.0\n",
            "  Downloading pillow-10.2.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Downloading pillow-10.2.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m148.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pillow\n",
            "Successfully installed pillow-10.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              },
              "id": "b2e96f31344f4ade948e24e8a0838cc8"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip uninstall -y pillow\n",
        "!pip install pillow==10.2.0 --force-reinstall --no-cache-dir\n",
        "!pip install gradio facenet-pytorch torch torchvision joblib gdown pillow numpy scikit-learn -q\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install facenet-pytorch\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import joblib\n",
        "from collections import Counter\n"
      ],
      "metadata": {
        "id": "z2SsUEcjE6PU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b6dddfa-2f66-4e21-d22b-1f38cc2f0c4e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: facenet-pytorch in /usr/local/lib/python3.12/dist-packages (2.6.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.24.0 in /usr/local/lib/python3.12/dist-packages (from facenet-pytorch) (1.26.4)\n",
            "Requirement already satisfied: Pillow<10.3.0,>=10.2.0 in /usr/local/lib/python3.12/dist-packages (from facenet-pytorch) (10.2.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from facenet-pytorch) (2.32.4)\n",
            "Requirement already satisfied: torch<2.3.0,>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from facenet-pytorch) (2.2.2)\n",
            "Requirement already satisfied: torchvision<0.18.0,>=0.17.0 in /usr/local/lib/python3.12/dist-packages (from facenet-pytorch) (0.17.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from facenet-pytorch) (4.67.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2025.8.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<2.3.0,>=2.2.0->facenet-pytorch) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<2.3.0,>=2.2.0->facenet-pytorch) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch<2.3.0,>=2.2.0->facenet-pytorch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import tarfile\n",
        "import os\n",
        "\n",
        "\n",
        "path = kagglehub.dataset_download(\"atulanandjha/lfwpeople\")\n",
        "print(\"‚úÖ Dataset downloaded to:\", path)\n",
        "\n",
        "tgz_path = os.path.join(path, \"lfw-funneled.tgz\")\n",
        "\n",
        "extract_path = \"/kaggle/working/lfw_funneled\"\n",
        "\n",
        "with tarfile.open(tgz_path, \"r:gz\") as tar:\n",
        "    tar.extractall(path=extract_path)\n",
        "\n",
        "print(\"‚úÖ Extraction done!\")\n",
        "print(\"üìÇ Number of folders:\", len(os.listdir(extract_path)))\n",
        "print(\"üìÇ First 5 people:\", os.listdir(extract_path)[:5])\n"
      ],
      "metadata": {
        "id": "JTiF8E2hBiSp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "709b5506-5eda-4399-824a-91b7fb72dbb1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'lfwpeople' dataset.\n",
            "‚úÖ Dataset downloaded to: /kaggle/input/lfwpeople\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2012395340.py:14: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(path=extract_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Extraction done!\n",
            "üìÇ Number of folders: 1\n",
            "üìÇ First 5 people: ['lfw_funneled']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"‚úÖ Using device:\", device)\n",
        "\n",
        "# Load FaceNet\n",
        "facenet = InceptionResnetV1(pretrained=\"vggface2\").eval().to(device)\n",
        "\n",
        "# Required transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((160, 160)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "def get_embedding(image_pil):\n",
        "    img_tensor = transform(image_pil).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        emb = facenet(img_tensor)\n",
        "    return emb.squeeze(0).cpu().numpy()\n"
      ],
      "metadata": {
        "id": "qnCSiEFhB3Jf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "072bb419-10d0-42dc-c0af-670792c79bb1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = [], []\n",
        "\n",
        "lfw_root = os.path.join(extract_path, \"lfw_funneled\")\n",
        "\n",
        "for person in os.listdir(lfw_root):\n",
        "    person_folder = os.path.join(lfw_root, person)\n",
        "    if not os.path.isdir(person_folder):\n",
        "        continue\n",
        "\n",
        "    for img_name in os.listdir(person_folder):\n",
        "        img_path = os.path.join(person_folder, img_name)\n",
        "        if os.path.isfile(img_path):\n",
        "            try:\n",
        "                img = Image.open(img_path).convert(\"RGB\")\n",
        "                emb = get_embedding(img)\n",
        "                X.append(emb)\n",
        "                y.append(person)\n",
        "            except Exception as e:\n",
        "                print(\"‚ùå Error with:\", img_path, e)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "print(\"‚úÖ Number of embeddings:\", X.shape)\n",
        "print(\"‚úÖ Number of people:\", len(set(y)))\n"
      ],
      "metadata": {
        "id": "454TWQdTCWUH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "584818b2-19e4-4b6d-c831-6468cc2f1c4d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Number of embeddings: (13233, 512)\n",
            "‚úÖ Number of people: 5749\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import joblib\n",
        "\n",
        "\n",
        "counts = Counter(y)\n",
        "mask = np.array([counts[label] > 1 for label in y])\n",
        "\n",
        "X_filtered = X[mask]\n",
        "y_filtered = y[mask]\n",
        "\n",
        "print(\"‚úÖ After filtering:\")\n",
        "print(\"Number of images:\", X_filtered.shape[0])\n",
        "print(\"Number of people:\", len(set(y_filtered)))\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_filtered, y_filtered, test_size=0.2, random_state=42, stratify=y_filtered\n",
        ")\n",
        "\n",
        "classifier = SVC(kernel=\"linear\", probability=True)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "joblib.dump(classifier, \"classifier.pkl\")\n",
        "print(\"‚úÖ Model trained and saved as classifier.pkl\")\n",
        "\n",
        "\n",
        "y_pred = classifier.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"üìä Accuracy on test set: {acc*100:.2f}%\")\n",
        "\n",
        "print(\"\\nüìë Classification Report (first 20 classes):\")\n",
        "print(classification_report(y_test, y_pred, labels=np.unique(y_test)[:20]))\n",
        "\n",
        "counts = Counter(y)\n",
        "mask = np.array([counts[label] > 1 for label in y])\n",
        "\n",
        "X_filtered = X[mask]\n",
        "y_filtered = y[mask]\n",
        "\n",
        "print(\"‚úÖ After filtering:\")\n",
        "print(\"Number of images:\", X_filtered.shape[0])\n",
        "print(\"Number of persons:\", len(set(y_filtered)))\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_filtered, y_filtered, test_size=0.2, random_state=42, stratify=y_filtered\n",
        ")\n",
        "\n",
        "\n",
        "classifier = SVC(kernel=\"linear\", probability=True)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "joblib.dump(classifier, \"classifier.pkl\")\n",
        "print(\"‚úÖ Model trained and saved as classifier.pkl\")\n",
        "\n",
        "\n",
        "y_pred = classifier.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"üìä Accuracy on test set: {acc*100:.2f}%\")\n",
        "\n",
        "print(\"\\nüìë Classification Report (first 20 classes):\")\n",
        "print(classification_report(y_test, y_pred, labels=np.unique(y_test)[:20]))\n"
      ],
      "metadata": {
        "id": "bFSMW5zECsKk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a9a9909-8f8a-45b0-9216-37010aee51fe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ After filtering:\n",
            "Number of images: 9164\n",
            "Number of people: 1680\n",
            "‚úÖ Model trained and saved as classifier.pkl\n",
            "üìä Accuracy on test set: 80.31%\n",
            "\n",
            "üìë Classification Report (first 20 classes):\n",
            "                              precision    recall  f1-score   support\n",
            "\n",
            "               Aaron_Peirsol       1.00      1.00      1.00         1\n",
            "         Abdel_Nasser_Assidi       0.00      0.00      0.00         1\n",
            "              Abdoulaye_Wade       1.00      1.00      1.00         1\n",
            "                    Abdullah       1.00      1.00      1.00         1\n",
            "                Abdullah_Gul       1.00      1.00      1.00         4\n",
            "         Abdullah_al-Attiyah       0.00      0.00      0.00         1\n",
            "                Abel_Pacheco       1.00      1.00      1.00         1\n",
            "Abid_Hamid_Mahmud_Al-Tikriti       0.00      0.00      0.00         1\n",
            "                Adam_Sandler       1.00      1.00      1.00         1\n",
            "              Adel_Al-Jubeir       0.00      0.00      0.00         1\n",
            "       Adolfo_Aguilar_Zinser       0.00      0.00      0.00         1\n",
            "                Adrien_Brody       1.00      1.00      1.00         3\n",
            "               Ahmed_Chalabi       1.00      1.00      1.00         1\n",
            "          Ahmet_Necdet_Sezer       0.00      0.00      0.00         1\n",
            "                 Ai_Sugiyama       1.00      1.00      1.00         1\n",
            "              Aicha_El_Ouafi       1.00      1.00      1.00         1\n",
            "    Akbar_Hashemi_Rafsanjani       0.00      0.00      0.00         1\n",
            "              Akhmed_Zakayev       1.00      1.00      1.00         1\n",
            "                     Al_Gore       1.00      1.00      1.00         2\n",
            "                   Al_Pacino       0.00      0.00      0.00         1\n",
            "\n",
            "                   micro avg       1.00      0.69      0.82        26\n",
            "                   macro avg       0.60      0.60      0.60        26\n",
            "                weighted avg       0.69      0.69      0.69        26\n",
            "\n",
            "‚úÖ After filtering:\n",
            "Number of images: 9164\n",
            "Number of persons: 1680\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model trained and saved as classifier.pkl\n",
            "üìä Accuracy on test set: 80.31%\n",
            "\n",
            "üìë Classification Report (first 20 classes):\n",
            "                              precision    recall  f1-score   support\n",
            "\n",
            "               Aaron_Peirsol       1.00      1.00      1.00         1\n",
            "         Abdel_Nasser_Assidi       0.00      0.00      0.00         1\n",
            "              Abdoulaye_Wade       1.00      1.00      1.00         1\n",
            "                    Abdullah       1.00      1.00      1.00         1\n",
            "                Abdullah_Gul       1.00      1.00      1.00         4\n",
            "         Abdullah_al-Attiyah       0.00      0.00      0.00         1\n",
            "                Abel_Pacheco       1.00      1.00      1.00         1\n",
            "Abid_Hamid_Mahmud_Al-Tikriti       0.00      0.00      0.00         1\n",
            "                Adam_Sandler       1.00      1.00      1.00         1\n",
            "              Adel_Al-Jubeir       0.00      0.00      0.00         1\n",
            "       Adolfo_Aguilar_Zinser       0.00      0.00      0.00         1\n",
            "                Adrien_Brody       1.00      1.00      1.00         3\n",
            "               Ahmed_Chalabi       1.00      1.00      1.00         1\n",
            "          Ahmet_Necdet_Sezer       0.00      0.00      0.00         1\n",
            "                 Ai_Sugiyama       1.00      1.00      1.00         1\n",
            "              Aicha_El_Ouafi       1.00      1.00      1.00         1\n",
            "    Akbar_Hashemi_Rafsanjani       0.00      0.00      0.00         1\n",
            "              Akhmed_Zakayev       1.00      1.00      1.00         1\n",
            "                     Al_Gore       1.00      1.00      1.00         2\n",
            "                   Al_Pacino       0.00      0.00      0.00         1\n",
            "\n",
            "                   micro avg       1.00      0.69      0.82        26\n",
            "                   macro avg       0.60      0.60      0.60        26\n",
            "                weighted avg       0.69      0.69      0.69        26\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "lfw_root = os.path.join(extract_path, \"lfw_funneled\")\n",
        "\n",
        "\n",
        "rand_idx = random.randint(0, len(X_test)-1)\n",
        "true_person = y_test[rand_idx]\n",
        "\n",
        "\n",
        "person_folder = os.path.join(lfw_root, true_person)\n",
        "if not os.path.exists(person_folder):\n",
        "  raise FileNotFoundError(f\"‚ùå Person {true_person} not found in {lfw_root}\")\n",
        "\n",
        "img_name = random.choice(os.listdir(person_folder))\n",
        "test_img_path = os.path.join(person_folder, img_name)\n",
        "\n",
        "\n",
        "test_img = Image.open(test_img_path).convert(\"RGB\")\n",
        "test_emb = get_embedding(test_img).reshape(1, -1)\n",
        "\n",
        "pred = classifier.predict(test_emb)[0]\n",
        "proba = classifier.predict_proba(test_emb).max()\n",
        "\n",
        "print(\"üñºÔ∏è Test image path:\", test_img_path)\n",
        "print(\"üñºÔ∏è True person:\", true_person)\n",
        "print(\"üîç Predicted:\", pred)\n",
        "\n"
      ],
      "metadata": {
        "id": "Z_X-35JmDM6o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5ca076d-c2b9-41de-8630-e01cbc6a1b61"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üñºÔ∏è Test image path: /kaggle/working/lfw_funneled/lfw_funneled/Sally_Kirkland/Sally_Kirkland_0002.jpg\n",
            "üñºÔ∏è True person: Sally_Kirkland\n",
            "üîç Predicted: Sally_Kirkland\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
        "import joblib\n",
        "import gdown\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Automatically download the trained model if not found\n",
        "# -----------------------------\n",
        "MODEL_PATH = \"classifier.pkl\"\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    gdown.download(\"https://drive.google.com/uc?id=1tYcltIFwMc6lx1f4z5ZkDcIpy6eETz3R\", MODEL_PATH, quiet=False)\n",
        "\n",
        "classifier = joblib.load(MODEL_PATH)\n",
        "\n",
        "# -----------------------------\n",
        "# Initialize FaceNet and MTCNN\n",
        "# -----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "mtcnn = MTCNN(keep_all=False, device=device)\n",
        "facenet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
        "\n",
        "# -----------------------------\n",
        "# Function to extract face embeddings and recognize the person\n",
        "# -----------------------------\n",
        "def recognize(image):\n",
        "    if image is None:\n",
        "        return \"‚ùå No image uploaded\"\n",
        "    img = image.convert(\"RGB\")\n",
        "    face = mtcnn(img)\n",
        "    if face is None:\n",
        "        return \"‚ùå No face detected\"\n",
        "    with torch.no_grad():\n",
        "        embedding = facenet(face.unsqueeze(0).to(device)).detach().cpu().numpy()\n",
        "    pred = classifier.predict(embedding)[0]\n",
        "    return f\"‚úÖ Person recognized: {pred}\"\n",
        "\n",
        "# -----------------------------\n",
        "# Gradio Interface\n",
        "# -----------------------------\n",
        "demo = gr.Interface(\n",
        "    fn=recognize,\n",
        "    inputs=gr.Image(type=\"pil\", label=\"Upload a face image\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Face Recognition App\",\n",
        "    description=\"Upload a photo and see who it is!\"\n",
        ")\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "badbDA3_-PFW",
        "outputId": "586abb58-f57b-4d9d-c400-f898318294fb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://01185056159e72bdea.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://01185056159e72bdea.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}